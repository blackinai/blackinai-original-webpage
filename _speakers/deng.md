---
layout: oral
display: Adji Bousso Dieng, Columbia University
type: oral
---
__Title__: A Recurrent Neural Network with Long-Range Semantic Dependency
<br><br>
__Abstract__: Language modeling is crucial to many NLP tasks. Applications include machine translation and speech recognition. Traditional n-gram and feed-forward neural network language models fail to capture long-range word dependencies. Previous work by Mikolov et al. has shown that adding context to a Recurrent Neural Network (RNN) language model is a promising direction to solve this issue. In this talk I will briefly review traditional language models and topic models before diving into the more recent contextual RNN-based language models. In particular, I will discuss the TopicRNN model, a RNN-based language model that captures long-range semantic dependencies using topic features. I will also highlight some results on word prediction and sentiment analysis using the TopicRNN model.
