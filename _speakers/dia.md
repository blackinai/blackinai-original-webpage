---
layout: oral
display: Ousmane Dia, ElementAI
type: oral
---
__Title__: Adversarial Functionality-Preserving Training in the Malware Domain
<br><br>
__Abstract__: Multiple approaches of generating adversarial examples have been proposed to deceive deep neural networks into predicting an incorrect target for a given observation [1, 2, 4, 7, 8, 10]. Most of the existing techniques that deal
with images involve either computing the gradients of a loss function with respect to the images pixels [3, 7, 10], or they inject some noise generally sampled from a random or a normal distribution [1, 4, 8] into a true case in the hope that the network will take an unexpected decision. While for images, the adversarial examples are generated in a way to be identical to the true cases, the precise locations of some details in a true image may still not be preserved in the perturbed one [2]. However, exact locations of those fine details are not usually important for perceptual image recognition or validation due to images high-entropy [6]. In Security, and specifically in malware detection, however, where the cases in hand usually consist of raw bytes or sequences of system calls,
this rarely holds. In Security, being able to generate new examples that preserve the functionalities (or malignant properties) of some true cases is paramount due to the difficulty of gathering large enough quantities of data for modeling purposes. We posit that the reasons the adversary generated examples may not preserve such properties are because the noise that is injected into the true cases is not necessarily sampled within the manifold of the true cases or that the gradients that are exploited are not selected in the neighborhood of the true examples.
<br>
In this study, we explore a new approach of generating adversarial malware cases. We make use of variational autoencoders (VAEs) (similar in spirit to [5]) to generate functionality-preserving mutations of true malware and extend Stein variational gradient descent [7] where the distribution of the latent samples are approximated using the true cases data-generating distribution. We also provide two ways to assess that the generated cases are functionality-preserving mutations of true malware: 1) by sampling sequences of bytes from the (vector representation of the) adversarial cases that we validate using as Oracle the Cuckoo Sandbox [9], and 2) by comparing specific sections of our generated mutations against true cases of malware. Because our architecture is generic enough, we evaluate our approach further with existing work on adversarial training of images and audio and compare our results.
