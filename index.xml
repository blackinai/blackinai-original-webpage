<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Black in AI on Black in AI</title>
    <link>https://esube.github.io/</link>
    <description>Recent content in Black in AI on Black in AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Black in AI</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>About BAI Workshops</title>
      <link>https://esube.github.io/workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/</guid>
      <description>&lt;p&gt;The second Black in AI event will be co-located with &lt;a href=&#34;https://nips.cc/&#34; target=&#34;_blank&#34;&gt;NIPS 2018&lt;/a&gt; at the &lt;a href=&#34;https://congresmtl.com/&#34; target=&#34;_blank&#34;&gt;Palais des Congrès de Montréal (Montreal Convention Centre)&lt;/a&gt;  in Montréal CANADA on December 7th from 9:00 am to 5:30 pm. The workshop will feature invited talks from prominent researchers and practitioners, oral presentations, and a poster session. There will also be socials to facilitate networking, discussion of different career opportunities in AI, and sharing of ideas to increase participation of Black researchers in the field. We invite all members of the AI community to attend the workshop. The early registration deadline for the conference is October 24, 2018. We strongly encourage workshop attendees to register for the workshops as early as possible. The registration can be found &lt;a href=&#34;https://nips.cc/accounts/login/?next=/Profile&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;important-dates&#34;&gt;Important Dates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;August 24, 2018: Travel grant application deadline&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;August 30, 2018: Abstract submission deadline&lt;/li&gt;
&lt;li&gt;September 21, 2018: Notification of travel grant acceptance&lt;/li&gt;
&lt;li&gt;September 30, 2018: Notification of submission acceptance&lt;/li&gt;
&lt;li&gt;October 24, 2018: NIPS early registration deadline&lt;/li&gt;
&lt;li&gt;December 7th, 2018: Workshop&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CFP 2017</title>
      <link>https://esube.github.io/workshop/2017/cfp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/cfp/</guid>
      <description>

&lt;p&gt;The first Black in AI event will be co-located with &lt;a href=&#34;https://nips.cc/&#34; target=&#34;_blank&#34;&gt;NIPS 2017&lt;/a&gt; in the &lt;strong&gt;Pike Ballroom&lt;/strong&gt; at the &lt;a href=&#34;http://www.marriott.com/hotels/travel/lgbrn-renaissance-long-beach-hotel&#34; target=&#34;_blank&#34;&gt;Renaissance Long Beach Hotel&lt;/a&gt;  in Long Beach, California on December 8th from 1:00pm to 6:00pm. The poster session will be held at the &lt;strong&gt;Broadlind 2&lt;/strong&gt; room on the second floor of the Renaissance hotel. The workshop will be followed by dinner 6:30-8:30. We invite Black AI researchers from around the world to share their work and learn about others’ research. The workshop will have invited talks from prominent researchers, oral presentations, and a poster session. There will also be socials to facilitate networking, discussion of different career opportunities in AI, and sharing of ideas to increase participation of Black researchers in the field. We invite all Black researchers, including undergraduates, graduate students, faculty, and researchers in industry to participate in this workshop. People of all races are also invited to attend the workshop to learn about the research being conducted by Black researchers across the world. Deadline for registration is October 31, 2017 and can done &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSfxDyUB0z--LwPNfK9ypOmNPMHf2wc51DuqSvituMtD53mGbA/viewform&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Please register as soon as possible to help us figure out headcount.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;important-dates&#34;&gt;Important Dates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;October 13, 2017: Abstract submission deadline&lt;/li&gt;
&lt;li&gt;October 13, 2017: Travel grant application deadline&lt;/li&gt;
&lt;li&gt;October 29, 2017: Notification of acceptance&lt;/li&gt;
&lt;li&gt;October 31, 2017: Workshop registration deadline&lt;/li&gt;
&lt;li&gt;December 8th from 1:00pm-5:30pm: Workshop&lt;/li&gt;
&lt;li&gt;December 8th from 6:30pm-9:00pm: Dinner&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CFP 2018</title>
      <link>https://esube.github.io/workshop/2018/cfp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/cfp/</guid>
      <description>&lt;p&gt;The second Black in AI event will be co-located with &lt;a href=&#34;https://nips.cc/&#34; target=&#34;_blank&#34;&gt;NIPS 2018&lt;/a&gt; at the &lt;a href=&#34;https://congresmtl.com/&#34; target=&#34;_blank&#34;&gt;Palais des Congrès de Montréal (Montreal Convention Centre)&lt;/a&gt;  in Montréal CANADA on December 7th from 9:00 am to 5:30 pm. The workshop will feature invited talks from prominent researchers and practitioners, oral presentations, and a poster session. There will also be socials to facilitate networking, discussion of different career opportunities in AI, and sharing of ideas to increase participation of Black researchers in the field. We invite all members of the AI community to attend the workshop. The early registration deadline for the conference is October 24, 2018. We strongly encourage workshop attendees to register for the workshops as early as possible. The registration can be found &lt;a href=&#34;https://nips.cc/accounts/login/?next=/Profile&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;important-dates&#34;&gt;Important Dates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;August 24th, 2018: Travel grant application deadline&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;August 30th, 2018: Abstract submission deadline&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;October 1st, 2018: Notification of travel grant acceptance&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;September 30th, 2018: Notification of submission acceptance&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;&lt;del&gt;October 24th, 2018: NIPS early registration deadline&lt;/del&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;December 7th, 2018: Workshop&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Organizers 2018</title>
      <link>https://esube.github.io/workshop/2017/organizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/organizers/</guid>
      <description>

&lt;h1 id=&#34;workshop-co-chairs&#34;&gt;Workshop Co-Chairs&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Rediet Abebe&lt;/strong&gt;, Cornell University&lt;br /&gt;
&lt;strong&gt;Sarah M. Brown&lt;/strong&gt;, University of California, Berkeley&lt;br /&gt;
&lt;strong&gt;Mouhamadou Moustapha Cisse&lt;/strong&gt;, Facebook AI Research&lt;br /&gt;
&lt;strong&gt;Timnit Gebru&lt;/strong&gt;, Microsoft Research&lt;br /&gt;
&lt;strong&gt;Sanmi Koyejo&lt;/strong&gt;, University of Illinois, Urbana-Champaign&lt;br /&gt;
&lt;strong&gt;Lyne P. Tchapmi&lt;/strong&gt;, Stanford University&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;2017-program-committee&#34;&gt;2017 Program Committee&lt;/h1&gt;

&lt;p&gt;Thanks to the following members of the Black in AI community and supportive allies for helping review the submissions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rediet Abebe&lt;/li&gt;
&lt;li&gt;Justice Amoh&lt;/li&gt;
&lt;li&gt;Silèye Ba&lt;/li&gt;
&lt;li&gt;Irwan Bello&lt;/li&gt;
&lt;li&gt;Samy Bengio&lt;/li&gt;
&lt;li&gt;Sarah M. Brown&lt;/li&gt;
&lt;li&gt;Joy Buolamwini&lt;/li&gt;
&lt;li&gt;Diana Cai&lt;/li&gt;
&lt;li&gt;Moustapha Cisse&lt;/li&gt;
&lt;li&gt;Charles Cearl&lt;/li&gt;
&lt;li&gt;Tewodros Dagnew&lt;/li&gt;
&lt;li&gt;Hal Daumé III&lt;/li&gt;
&lt;li&gt;Ousmane Dia&lt;/li&gt;
&lt;li&gt;Ashley Edwards&lt;/li&gt;
&lt;li&gt;Oluwaseun Francis Egbelowo&lt;/li&gt;
&lt;li&gt;Dylan Foster&lt;/li&gt;
&lt;li&gt;Fisseha Gidey Gebremedhin&lt;/li&gt;
&lt;li&gt;Timnit Gebru&lt;/li&gt;
&lt;li&gt;Christan Grant&lt;/li&gt;
&lt;li&gt;Alvin Grissom II&lt;/li&gt;
&lt;li&gt;Bernease Herman&lt;/li&gt;
&lt;li&gt;Jack Hessel&lt;/li&gt;
&lt;li&gt;Abigail Jacobs&lt;/li&gt;
&lt;li&gt;Emmanuel Johnson&lt;/li&gt;
&lt;li&gt;Sanmi Koyejo&lt;/li&gt;
&lt;li&gt;Ciira Maina&lt;/li&gt;
&lt;li&gt;nyalleng moorosi&lt;/li&gt;
&lt;li&gt;George Musumba&lt;/li&gt;
&lt;li&gt;Ndapa Nakashole&lt;/li&gt;
&lt;li&gt;Ehi Nosakhare&lt;/li&gt;
&lt;li&gt;Billy Okal&lt;/li&gt;
&lt;li&gt;Charles Onu&lt;/li&gt;
&lt;li&gt;Forough Poursabzi-Sangdeh&lt;/li&gt;
&lt;li&gt;Alexandra Schofield&lt;/li&gt;
&lt;li&gt;Frank Lanke Fu Tarimo&lt;/li&gt;
&lt;li&gt;Lyne P. Tchapmi&lt;/li&gt;
&lt;li&gt;Kale-ab Tessera&lt;/li&gt;
&lt;li&gt;Basiliyos Tilahun BETRU&lt;/li&gt;
&lt;li&gt;Wil Thomason&lt;/li&gt;
&lt;li&gt;Marcelo Worsley&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Organizers 2018</title>
      <link>https://esube.github.io/workshop/2018/organizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/organizers/</guid>
      <description>

&lt;h2 id=&#34;workshop-co-chairs&#34;&gt;Workshop Co-Chairs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~red/&#34; target=&#34;_blank&#34;&gt;Rediet Abebe&lt;/a&gt;, Cornell University&lt;br /&gt;
&lt;a href=&#34;https://esube.github.io&#34; target=&#34;_blank&#34;&gt;Esube Bekele&lt;/a&gt;, US Naval Research Laboratory&lt;br /&gt;
&lt;a href=&#34;http://sarahmbrown.org/&#34; target=&#34;_blank&#34;&gt;Sarah Brown&lt;/a&gt;, Brown University&lt;br /&gt;
&lt;a href=&#34;http://moustaphacisse.com/&#34; target=&#34;_blank&#34;&gt;Mouhamadou Moustapha Cisse&lt;/a&gt;, Google AI&lt;br /&gt;
&lt;a href=&#34;http://ai.stanford.edu/~tgebru/&#34; target=&#34;_blank&#34;&gt;Timnit Gebru&lt;/a&gt;, Google AI&lt;br /&gt;
&lt;a href=&#34;http://www.gichoya.me/about/&#34; target=&#34;_blank&#34;&gt;Judy Wawira Gichoya&lt;/a&gt;, Oregon Health and Science University&lt;br /&gt;
&lt;a href=&#34;https://www.linkedin.com/in/devin-guillory-78528958/&#34; target=&#34;_blank&#34;&gt;Devin Guillory&lt;/a&gt;, Etsy&lt;br /&gt;
&lt;a href=&#34;http://sanmi.cs.illinois.edu/&#34; target=&#34;_blank&#34;&gt;Sanmi Koyejo&lt;/a&gt;, University of Illinois, Urbana Champaigns&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/ezinne-nwankwo-119586101/&#34; target=&#34;_blank&#34;&gt;Ezinne Nwankwo&lt;/a&gt;, Harvard University&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/mohamed-hassan-kane-4b50328a/&#34; target=&#34;_blank&#34;&gt;Hassan Kane&lt;/a&gt;, Sela Labs&lt;/p&gt;

&lt;h2 id=&#34;2018-program-committee&#34;&gt;2018 Program Committee&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Institution / Company&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Rediet Abebe&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Dhaval Adjodah&lt;/td&gt;
&lt;td&gt;MIT Media Lab&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yoseph Berhanu Alebachew&lt;/td&gt;
&lt;td&gt;Addis Ababa University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Justice Amoh&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Amar Ashar&lt;/td&gt;
&lt;td&gt;Berkman Klein Center for Internet &amp;amp; Society&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sileye Ba&lt;/td&gt;
&lt;td&gt;Dailymotion&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Esube Bekele&lt;/td&gt;
&lt;td&gt;US Naval Research Laboratory&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ayalew Belay&lt;/td&gt;
&lt;td&gt;Addis Ababa University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Danielle Belgrave&lt;/td&gt;
&lt;td&gt;Microsoft Research&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Irwan Bello&lt;/td&gt;
&lt;td&gt;Google Brain&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Samy Bengio&lt;/td&gt;
&lt;td&gt;Google Brain&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Basiliyos Tilahun Betru&lt;/td&gt;
&lt;td&gt;National Advanced School of Engineering University Of Yaounde I&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;David Bindel&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sarah Brown&lt;/td&gt;
&lt;td&gt;Brown University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Diana Cai&lt;/td&gt;
&lt;td&gt;Princeton University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Mouhamadou Moustapha Cisse&lt;/td&gt;
&lt;td&gt;Google AI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Alexis Cook&lt;/td&gt;
&lt;td&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hal Daumé III&lt;/td&gt;
&lt;td&gt;Microsoft Research / University of Maryland&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lucio Dery&lt;/td&gt;
&lt;td&gt;Stanford / Facebook&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ousmane Amadou Dia&lt;/td&gt;
&lt;td&gt;Element AI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Victor C Dibia&lt;/td&gt;
&lt;td&gt;IBM Research&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adji Bousso Dieng&lt;/td&gt;
&lt;td&gt;Columbia University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Awa Dieng&lt;/td&gt;
&lt;td&gt;Duke University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Charles Earl&lt;/td&gt;
&lt;td&gt;Automattic&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ashley Edwards&lt;/td&gt;
&lt;td&gt;Georgia Institute of Technology&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Oluwaseun Francis Egbelowo&lt;/td&gt;
&lt;td&gt;University of the Witwatersrand&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ruth Fong&lt;/td&gt;
&lt;td&gt;University of Oxford&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sorelle Friedler&lt;/td&gt;
&lt;td&gt;Haverford College&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lanke Frank Fu&lt;/td&gt;
&lt;td&gt;Ascent Robotics&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Timnit Gebru&lt;/td&gt;
&lt;td&gt;Google AI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Judy Wawira Gichoya&lt;/td&gt;
&lt;td&gt;Oregon Health and Science University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Gebremedhin Fisseha Gidey&lt;/td&gt;
&lt;td&gt;National Advanced School of Engineering University Of Yaounde I&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Solomon Gizaw&lt;/td&gt;
&lt;td&gt;Addis Ababa University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ian Goodfellow&lt;/td&gt;
&lt;td&gt;Google Brain&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Christan Grant&lt;/td&gt;
&lt;td&gt;University of Oklahoma&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ben Green&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Alvin Grissom II&lt;/td&gt;
&lt;td&gt;Ursinus College&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Devin Guillory&lt;/td&gt;
&lt;td&gt;Etsy&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Bernease Herman&lt;/td&gt;
&lt;td&gt;University of Washington&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jack Hessel&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lily Hu&lt;/td&gt;
&lt;td&gt;Harvard University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Abigail Jacobs&lt;/td&gt;
&lt;td&gt;University of California Berkeley&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Emmanuel Johnson&lt;/td&gt;
&lt;td&gt;University of Southern California&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sanmi Koyejo&lt;/td&gt;
&lt;td&gt;University of Illinois at Urbana-Champaign&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hugo Larochelle&lt;/td&gt;
&lt;td&gt;Google Brain&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Zachary C Lipton&lt;/td&gt;
&lt;td&gt;Carnegie Mellon University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Kristian Lum&lt;/td&gt;
&lt;td&gt;Human Rights and Data Analysis Group&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ciira Maina&lt;/td&gt;
&lt;td&gt;Dedan Kimathi University of Technology&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Vukosi Marivate&lt;/td&gt;
&lt;td&gt;University of Pretoria&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Margaret Mitchell&lt;/td&gt;
&lt;td&gt;Google AI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nyalleng Moorosi&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;George Musumba&lt;/td&gt;
&lt;td&gt;Dedan Kimathi University of Technology&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Zanele Munyikwa&lt;/td&gt;
&lt;td&gt;MIT Sloan School of Management&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Robert Ness&lt;/td&gt;
&lt;td&gt;Gamalon Inc&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ehi Nosakhare&lt;/td&gt;
&lt;td&gt;MIT&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Billy Okal&lt;/td&gt;
&lt;td&gt;Voyage&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Charles C Onu&lt;/td&gt;
&lt;td&gt;McGill University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Vicente Ordonez&lt;/td&gt;
&lt;td&gt;University of Virginia&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lonnie T Parker&lt;/td&gt;
&lt;td&gt;Naval Sea Systems Command&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Forough Poursabzi-Sangdeh&lt;/td&gt;
&lt;td&gt;Microsoft Research&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;David Robinson&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Evan Rosenman&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Negar Rostamzadeh&lt;/td&gt;
&lt;td&gt;Element AI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Natalie Schluter&lt;/td&gt;
&lt;td&gt;IT University of Copenhagen&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Flora Tasse&lt;/td&gt;
&lt;td&gt;University of Cambridge&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rachael Tatman&lt;/td&gt;
&lt;td&gt;Kaggle&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Matthew Tesfaldet&lt;/td&gt;
&lt;td&gt;York University &amp;amp; Vector Institute of Artificial Intelligence&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Alemu Leulseged Tesfaye&lt;/td&gt;
&lt;td&gt;Ca Foscari University of Venice&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Kale-ab Tessera&lt;/td&gt;
&lt;td&gt;University of Witwatersrand&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rachel Thomas&lt;/td&gt;
&lt;td&gt;fast.ai &amp;amp; University of San Francisco&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Wil Thomason&lt;/td&gt;
&lt;td&gt;Cornell University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Johan Ugander&lt;/td&gt;
&lt;td&gt;Stanford University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jenn Wortman Vaughan&lt;/td&gt;
&lt;td&gt;Microsoft Research&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Melissa Woghiren&lt;/td&gt;
&lt;td&gt;University of Alberta&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Marcelo Worsley&lt;/td&gt;
&lt;td&gt;Northwestern University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Tao Xie&lt;/td&gt;
&lt;td&gt;University of Illinois at Urbana-Champaign&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jason Yosinski&lt;/td&gt;
&lt;td&gt;Uber AI Labs&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Amsale Zelalem&lt;/td&gt;
&lt;td&gt;Addis Ababa University&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Programs 2017</title>
      <link>https://esube.github.io/workshop/2017/programs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/programs/</guid>
      <description>

&lt;h2 id=&#34;schedule-2017&#34;&gt;Schedule 2017&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1:00 - 1:05 pm&lt;/td&gt;
&lt;td&gt;Introduction&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1:25 - 1:55 pm&lt;/td&gt;
&lt;td&gt;Keynote Address&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1:55 - 2:15 pm&lt;/td&gt;
&lt;td&gt;Oral Session 1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2:15 - 4:15 pm&lt;/td&gt;
&lt;td&gt;Keynote Address&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4:15 - 4:35 pm&lt;/td&gt;
&lt;td&gt;Poster Session &amp;amp; Coffee Break&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4:35 - 5:05 pm&lt;/td&gt;
&lt;td&gt;Keynote  Address&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5:05 - 5:25 pm&lt;/td&gt;
&lt;td&gt;Oral Session 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5:25 - 5:55 pm&lt;/td&gt;
&lt;td&gt;Keynote Address&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5:55 - 6:00 pm&lt;/td&gt;
&lt;td&gt;Panel&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6:00 - 6:30 pm&lt;/td&gt;
&lt;td&gt;Closing Remarks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6:30 - 9:00 pm&lt;/td&gt;
&lt;td&gt;Dinner&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;keynotes&#34;&gt;Keynotes&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Speakers&lt;/th&gt;
&lt;th&gt;Bio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/cwamainadekut/&#34; target=&#34;_blank&#34;&gt;Ciira Maina&lt;/a&gt;, Dedan Kimathi University of Technology&lt;br&gt;&lt;em&gt;Leveraging Machine Learning, Low Cost Devices and Open Science for Impact in the Developing World: An Example In Ecology&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Ciira Maina graduated from the University of Nairobi, Kenya with a Bsc. degree in Electrical Engineering (First class honors) in 2007 and with a Ph.D. from Drexel University in Philadelphia, USA in September 2011. At Drexel he was a member of the Adaptive Signal Processing and Information Theory Research Group where he conducted research on robust speech processing. Between October 2011 and August 2013 he was a postdoctoral researcher in computational Biology working with Prof. Magnus Rattray and Prof. Neil Lawrence at the University of Sheffield. Since September 2013 he has been a Lecturer in Electrical Engineering at Dedan Kimathi University of Technology in Nyeri, Kenya where he conducts research on bioacoustic approaches to environmental monitoring, sensor systems for livestock monitoring and novel approaches to electrical engineering instruction. In addition he serves on the organising committee for Data Science Africa, an organisation that runs an annual data science and machine learning summer school and workshop in Africa.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;http://www.imperial.ac.uk/people/d.belgrave&#34; target=&#34;_blank&#34;&gt;Danielle Belgrave&lt;/a&gt;, Microsoft Research&lt;br&gt;&lt;em&gt;Machine Learning for Personalised Health&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Danielle Belgrave is a Researcher at Microsoft Research Cambridge. She also holds a tenured Research Fellowship (Assistant Professor) at Imperial College London. Her research focuses on developing probabilistic and causal graphical modelling frameworks to understand disease progression over time. The aim of this research is to use machine learning to identify distinct subtypes of disease evolution (endotypes) and to understand the underlying mechanisms of these subtypes so as to develop personalized disease management strategies. She has a BSc in Business Mathematics and Statistics from the London School of Economics and an MSc in Statistics from University College London. She was awarded a Microsoft PhD Scholarship to complete her PhD in Statistics and Machine Learning applied to Health (2010-2013) at The University of Manchester.  She received a Medical Research Council (UK) Career Development Award in Biostatistics (2015 – 2020) for the project “Unified probabilistic latent variable modelling strategies to accelerate endotype discovery in longitudinal studies”.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/dolaoseb/&#34; target=&#34;_blank&#34;&gt;Debo Olaosebikan&lt;/a&gt;, Gigster&lt;br&gt;&lt;em&gt;How to Automate the Creation of Software&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Debo is co-founder and CTO of &lt;a href=&#34;https://gigster.com/&#34; target=&#34;_blank&#34;&gt;Gigster&lt;/a&gt;, a software development marketplace that seeks to automate the creation and delivery of software while creating a productive workplace of the future for engineers. Gigster logs data about code, projects, and people throughout the software development lifecycle and uses patterns in that data to drive increases in reliability and efficiency. Gigster aims to apply machine learning to challenging problems like software cost and time estimation, optimal team formation, predicting the future (risk) on projects, and ultimately code generation.  Gigster is backed by Andreessen Horowitz, Redpoint, Y Combinator, and Greylock.Debo has founded multiple marketplace, energy, and data startups. He is on leave from a physics PhD at Cornell where he worked on silicon nanophotonics and theoretical physics. He was once a radio featured musician and was the young Nigerian scientist of 2011. Debo advises startups and helps young founders as a Thiel Fellowship mentor.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://habengirma.com/&#34; target=&#34;_blank&#34;&gt;Haben Girma&lt;/a&gt;, Harvard Law School&lt;br&gt;&lt;em&gt;Disability and Innovation: the benefits of universal design&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;The first Deafblind person to graduate from Harvard Law School, Haben Girma advocates for equal opportunities for people with disabilities. President Obama named her a White House Champion of Change, and Forbes recognized her in Forbes 30 Under 30. Haben travels the world consulting and public speaking, teaching clients the benefits of fully accessible products and services. Haben is a talented storyteller who helps people frame difference as an asset. She resisted society’s low expectations, choosing to create her own pioneering story. Because of her disability rights advocacy she has been honored by President Obama, President Clinton, and many others. Haben is also writing a memoir that will be published by Grand Central Publishing in 2019.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;oral-research-presenters&#34;&gt;Oral Research Presenters&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Researcher&lt;/th&gt;
&lt;th&gt;Abstract&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bonolo Mathibel&lt;br&gt;IBM Research Africa&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Towards Impactful Artificial Intelligence on the African Continent&lt;/em&gt;&lt;br&gt;In recent years, machine learning has been applied to solve diverse sets of challenges on the African continent. This includes reducing road traffic congestion in the face of failing road infrastructure in South Africa, drought modeling in the Horn of Africa, transfer learning for cassava disease detection in sub-Saharan Africa, and galaxy count extraction from radio telescopes. The vast majority of research conducted in the field of Artificial Intelligence (AI) occurs outside of the African continent, and the few studies that have been applied to the African context are based on bespoke datasets generated to solve the problem at hand. We therefore propose three pillars of representation that are foundational to achieving impactful, sustainable, and scalable AI research and product development for and on the African continent. Our aim is to increase the number of AI studies conducted in Africa and encourage researchers and AI practitioners to consider both science and impact when selecting problems to work on.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;George W Musumba&lt;br&gt;Dedan Kimathi University of Technology&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Modelling Virtual Enterprises Using a Multi-Agent Systems Approach&lt;/em&gt; &lt;br&gt; Nowadays enterprises work together towards a common goal by sharing responsibilities and profits as is the case for construction related projects. The construction sector’s potential contribution to the economic growth of developing countries can be enhanced if the challenges facing the sector that include delayed completion of projects, frequent collapse of buildings, lack of ethics, incompetent design, use of inappropriate materials, poor coordination and management of contractors are effectively addressed. These can be attributed to poor choice of partner enterprises for the tasks due to insufficient information available about them and lack of facilitation techniques. Selection of best partner among many for construction project is a Multi-Criteria Decision Making (MCDM) process. Existing MCDM techniques cannot be used to select right partners for construction projects. Fuzzy Analytical Hierarchy Process (FAHP) and Group Fuzzy Analytical Hierarchy Process (GFAHP), MCDM algorithms that learns partner attributes (machine learning technique incorporated), were designed and applied. A Multi-Agent Systems(MAS) approach was used for simulations. The approach provide efficient decision-making support for human beings using software agents. Results show that this technique is both efficient and effective. Validation of the system, carried out by stakeholders, show that it is approximately 99.7% accurate in the evaluation and selection of partners and partners&amp;rsquo;s performance evaluation.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Charles Onu&lt;br&gt;McGill University&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Saving Newborn Lives at Birth through Machine Learning&lt;/em&gt; &lt;br&gt;Every year, 3 million newborns die within the first month of life. Birth asphyxia and other breathing-related conditions are a leading cause of mortality during the neonatal phase. Current diagnostic methods are too sophisticated in terms of equipment, required expertise, and general logistics. Consequently, early detection of asphyxia in newborns is very difficult in many parts of the world, especially in resource-poor settings. We are developing a machine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia through automated analysis of the infant cry. Deployed via smartphone and wearable technology, Ubenwa will drastically reduce the time, cost and skill required to make accurate and potentially life-saving diagnoses.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ousmane Dia&lt;br&gt;ElementAI&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Adversarial Functionality-Preserving Training in the Malware Domain&lt;/em&gt; &lt;br&gt;Multiple approaches of generating adversarial examples have been proposed to deceive deep neural networks into predicting an incorrect target for a given observation [1, 2, 4, 7, 8, 10]. Most of the existing techniques that deal with images involve either computing the gradients of a loss function with respect to the images pixels [3, 7, 10], or they inject some noise generally sampled from a random or a normal distribution [1, 4, 8] into a true case in the hope that the network will take an unexpected decision. While for images, the adversarial examples are generated in a way to be identical to the true cases, the precise locations of some details in a true image may still not be preserved in the perturbed one [2]. However, exact locations of those fine details are not usually important for perceptual image recognition or validation due to images high-entropy [6]. In Security, and specifically in malware detection, however, where the cases in hand usually consist of raw bytes or sequences of system calls, this rarely holds. In Security, being able to generate new examples that preserve the functionalities (or malignant properties) of some true cases is paramount due to the difficulty of gathering large enough quantities of data for modeling purposes. We posit that the reasons the adversary generated examples may not preserve such properties are because the noise that is injected into the true cases is not necessarily sampled within the manifold of the true cases or that the gradients that are exploited are not selected in the neighborhood of the true examples.&lt;br&gt;In this study, we explore a new approach of generating adversarial malware cases. We make use of variational autoencoders (VAEs) (similar in spirit to [5]) to generate functionality-preserving mutations of true malware and extend Stein variational gradient descent [7] where the distribution of the latent samples are approximated using the true cases data-generating distribution. We also provide two ways to assess that the generated cases are functionality-preserving mutations of true malware: 1) by sampling sequences of bytes from the (vector representation of the) adversarial cases that we validate using as Oracle the Cuckoo Sandbox [9], and 2) by comparing specific sections of our generated mutations against true cases of malware. Because our architecture is generic enough, we evaluate our approach further with existing work on adversarial training of images and audio and compare our results.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adji Bousso Dieng&lt;br&gt;Columbia University&lt;/td&gt;
&lt;td&gt;&lt;em&gt;A Recurrent Neural Network with Long-Range Semantic Dependency&lt;/em&gt;&lt;br&gt; Language modeling is crucial to many NLP tasks. Applications include machine translation and speech recognition. Traditional n-gram and feed-forward neural network language models fail to capture long-range word dependencies. Previous work by Mikolov et al. has shown that adding context to a Recurrent Neural Network (RNN) language model is a promising direction to solve this issue. In this talk I will briefly review traditional language models and topic models before diving into the more recent contextual RNN-based language models. In particular, I will discuss the TopicRNN model, a RNN-based language model that captures long-range semantic dependencies using topic features. I will also highlight some results on word prediction and sentiment analysis using the TopicRNN model.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Flora Ponjou Tasse&lt;br&gt;University of Cambridge&lt;/td&gt;
&lt;td&gt;&lt;em&gt;ShapeSearch: A Generic Engine for 3D Models, Images, and Sketches&lt;/em&gt;&lt;br&gt;We present ShapeSearch, a generic search engine for shapes that supports queries such as 3D models, images, sketches, and text. Online repositories of images and 3D objects are growing at an exponential rate, used by growing communities of makers and artists. Moreover, the proliferation of Augmented Reality platforms is creating new communities of content creators and developers in need of 3D content. However, search features in the large 3D repositories are still limited to text. On the other hand, the research community has made significant progress in context-based shape retrieval, but current methods are typically limited to one modality such as images or sketches. We propose a generic search engine able to retrieve relevant shapes based on a wide range of modalities by leveraging the latest machine learning advances in Graphics, Vision, and NLP.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;accepted-posters&#34;&gt;Accepted Posters&lt;/h2&gt;

&lt;p style=&#34;color: black&#34;&gt;
(More posters will be added soon)

&lt;br&gt;&lt;br&gt;
1.   AI Powered Process Improvement, Christine Custis*, NewPearl, Inc.
&lt;br&gt;&lt;br&gt;
2.   Morphological classification of Radio Sources and thier Counterparts in Optical using Deep Machine Learning, Superviser: Prof R. Taylor, Wathela Alhassan*, University of Cape Town
&lt;br&gt;&lt;br&gt;
3.   Orchestra Mobile Crowdsensing and Computing Platform: A Roadmap for Further Development, Sando George*, Warsaw University of Technology; Maria Ganzha, Warsaw University of Technology; Marcin Paprzycki, Systems Research Institute, Polish Academy of Sciences&lt;br&gt;&lt;br&gt;
4.   Using Dominant Sets for Data Association in Multi-Camera Tracking, Kedir Hamid Ahmed*, Ethiopian Bio technology Institute
&lt;br&gt;&lt;br&gt;
5.   Churn Prediction using Structured Logical Knowledge and Convolutional Neural Networks, Gridach Mourad*, High Institute of Technology - Agadir
&lt;br&gt;&lt;br&gt;
6.   Evolving Realistic 3D Facial Expressions using Interactive Genetic Algorithms, Meareg Hailemariam*, Hanson Robtotics/Labs iCog
&lt;br&gt;&lt;br&gt;
7.   Amharic-English Speech Translation, Michael Woldeyohannis*, Addis Ababa University, Addis Ababa, Ethiopia; Million Meshesha, Addis Ababa University; Laurent Besacier, LIG, Univ. Grenoble Alpes
&lt;br&gt;&lt;br&gt;
8.   Machine Learning Approach On Detection of Privilege Escalation Attacks in Android Smartphones, Bruno Ssekiwere*, Uganda Technology and Management University
&lt;br&gt;&lt;br&gt;
9.   A signature-based Denial of Service and Probe detector model based on data mining techniques, Claire Babirye*, Uganda Technology and Management University; Ernest Mwebaze, Uganda Technology and Management University
&lt;br&gt;&lt;br&gt;
10.   Modelling Virtual Enterprises Using a Multi-Agent Systems Approach, George Musumba*, Dedan Kimathi University of Technology
&lt;br&gt;&lt;br&gt;
11.   Behavioural Multi-Factor Authentication Using Keystroke Dynamics, Roy Henha Eyono*, University of Cape Town
&lt;br&gt;&lt;br&gt;
12.   Feature Extraction and Selection of Optical Galaxy Data, Roy Henha Eyono*, University of Cape Town
&lt;br&gt;&lt;br&gt;
13.   Compressive Sampling for Phenotype Classification, Eric Brooks*, Air Force
&lt;br&gt;&lt;br&gt;
14.   An iterative Dynamic Game Approach for Robust Deep Reinforcement Learning, Olalekan Ogunmolu*, University of Texas at Dallas; Nicholas Gans, UT Dallas; Tyler Summers, UT Dallas
&lt;br&gt;&lt;br&gt;
15.   Saving Newborn Lives at Birth through Machine Learning, Charles Onu*, McGill University
&lt;br&gt;&lt;br&gt;
16.   Predicting Road Traffic Accident Severity: A Small Case Study in South Africa, Mpho Mokoatle*, CSIR; Vukosi Marivate, CSIR
&lt;br&gt;&lt;br&gt;
17.   ShapeSearch: a generic search engine for 3D models, images and sketches, Flora Ponjou Tasse*, University of Cambridge
&lt;br&gt;&lt;br&gt;
18.   ZCal: Machine learning for calibrating radio interferometric data., Simphiwe Zitha*, Rhodes university &amp; SKA-SA
&lt;br&gt;&lt;br&gt;
19.   A translation-based approach to the learning of the morphology of an under-resourced language, Tewodros Gebreselassie*, Addis Ababa University; Michael Gasser, Indiana University
&lt;br&gt;&lt;br&gt;
20.   Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs, Adil SALIM*, Telecom ParisTech; Pascal BIANCHI, Telecom ParisTech; Walid HACHEM, Universite Paris-Est Marne-la-Vallee
&lt;br&gt;&lt;br&gt;
21.   Orthographic Representation Learning for Modeling Dyslexia, HENRY WOLF VII*, University of Connecticut
&lt;br&gt;&lt;br&gt;
22.   Enhanced Robustness in Speech Emotion Recognition: using Acoustic and Linguistic Features, hana tisasu*, iCog-Labs
&lt;br&gt;&lt;br&gt;
23.   Semi-Supervised Learning in Brain Imaging Data for Classification of Schizophrenia, Tewodros Dagnew*, Università degli studi di milano
&lt;br&gt;&lt;br&gt;
24.   Language Guided Pixel-Space Planning, Emmanuel Kahembwe*, Edinburgh University
&lt;br&gt;&lt;br&gt;
25.   The UMD Neural Machine Translation Systemsat WMT17 Bandit Learning Task, kiante brantley*, The University of Maryland College Park
&lt;br&gt;&lt;br&gt;
26.   FPGA-Based CNN Processor Utilizing Parallel Feature Processing And Pseudo Parallel Memories, Muluken Hailesellasie*, Tennessee Tech.
&lt;br&gt;&lt;br&gt;
27.   Weakly Supervised Classification in High Energy Physics, Lucio Dery*, Stanford University
&lt;br&gt;&lt;br&gt;
28.   Prediction of neuropsychiatric conditions through switch detection in fluency tasks, Felipe Paula*, Federal University of Rio Grande do Sul - UFRGS; Rodrigo Wilkens, Université Catholique de Louvain - CENTAL; Marco Idiart, Federal University of Rio Grande do Sul - UFRGS; Aline Villavicencio, Federal University of Rio Grande do Sul - UFRGS
&lt;br&gt;&lt;br&gt;
29.   DETECTION OF ULCERS FROM CAPSULE ENDOSCOPIC IMAGES USING CONVOLUTIONAL NEURAL NETWORKS, Isa Nuruddeen*, Makerere University Uganda
&lt;br&gt;&lt;br&gt;
30.   Intelligent License Plate Recognition and Reporting, Yaecob Girmay Gezahegn, Addis Ababa University; Misgina Tsighe Hagos*, Ethiopian Biotechnology Institute; Dereje H.Mariam W.Gebreal, Addis Ababa University; Teklay GebreSlassie Zeferu, Addis Ababa University; G.agziabher Ngusse G.Tekle, Addis Ababa University; Yakob Kiros T.Haimanot, Mekelle University
&lt;br&gt;&lt;br&gt;
31.   MODELLING CONTEXT FOR A DEEP RECURRENT NEURAL NETWORK LANGUAGE MODEL, Linda Khumalo*, University of the Witwatersrand
&lt;br&gt;&lt;br&gt;
32.   Convolutional Sequence to Sequence Learning, Yann Dauphin*, Facebook
&lt;br&gt;&lt;br&gt;
33.   Integrating Attention Model into Hierarchical Recurrent Encoder-Decoder to Improve Dialogue Response Generation, Oluwatobi Olabiyi*, Capital One; Erik Mueller, Capital One
&lt;br&gt;&lt;br&gt;
34.   Advantages of Deep Learning Techniques on Grayscale Radiographs, Obioma Pelka*, University of Applied Sciences and Arts Dortmund
&lt;br&gt;&lt;br&gt;
35.   Hybrid Intelligent System for Lung Cancer Type Identification, yenatfanta Bayleyegn*, Ethiopian Biotechnology Institute; Kumudha Raimond, Karunya University
&lt;br&gt;&lt;br&gt;
36.   Towards impactful artificial intelligence on the African continent, Bonolo Mathibela*, IBM Research
&lt;br&gt;&lt;br&gt;
37.   Soft-biometrics Attributes Multi-Label Classification with Deep Residual Networks, Esube Bekele*, US Naval Research Lab; Wallace Lawson, Naval Research Laboratory
&lt;br&gt;&lt;br&gt;
38.   Learning an Interactive Attention Policy for Neural Machine Translation, Samee Ibraheem*, UC Berkeley
&lt;br&gt;&lt;br&gt;
39.   Ubiquitous Monitoring of Abnormal Respiratory Sounds, Justice Amoh*, Dartmouth College
&lt;br&gt;&lt;br&gt;
40.   Question Arbitration for Robot Task Learning, Kalesha Bullard*, Georgia Institute of Technology
&lt;br&gt;&lt;br&gt;
41.   Cluster-based Approach to Improve Affect Recognition from Passively Sensed Data, Mawulolo Ameko*, University of Virginia
&lt;br&gt;&lt;br&gt;
42.   Gaze and Voice as an Input Tool for Software Interfaces, Timothy Mwiti*, NORTHWESTERN UNIVERSITY
&lt;br&gt;&lt;br&gt;
43.   Transferring Agent Behaviors from Videos via Motion GANs, Ashley Edwards*, Georgia Institute of Technology; Charles Isbell, Georgia Institute of Technology
&lt;br&gt;&lt;br&gt;
44.   TopicRNN: A Recurrent Neural Network With Long-Range Semantic Dependency, Adji Bousso Dieng*, Columbia University
&lt;br&gt;&lt;br&gt;
45.   Probabilistic Multi-view based Diagnosis and Anomaly Detection of Sensors in Weather Station, Tadesse Zemicheal*, Oregon State University
&lt;br&gt;&lt;br&gt;
46.   Reinforcement Learning-based Simultaneous Translation with Final Verb Prediction, Alvin Grissom II*, Ursinus College
&lt;br&gt;&lt;br&gt;
47.   Towards a real-time in-seat activity tracker, Austin Little*, Georgia Institute of Technology
&lt;br&gt;&lt;br&gt;
48.   Robust Visual 6D Pose Tracking Using Learned Dense Data Association, Lanke Frank Tarimo Fu*, Independent Researcher (Formerly ETH Zurich)
&lt;br&gt;&lt;br&gt;
49.   An Ensemble-based Approach to Click-Through Rate Prediction for Promoted Listings at Etsy, Devin Guillory*, Etsy
&lt;br&gt;&lt;br&gt;
50.   Fluorescence Bioimaging of Organellar Network Evolution, Chinasa Okolo*, Pomona College
&lt;br&gt;&lt;br&gt;
51.   Intersectional Phenotypic and Demographic Evaluation of Gender Classification, Joy Buolamwini*, MIT
&lt;br&gt;&lt;br&gt;
52.   Generalizable Intention Prediction of Human Drivers at Intersections, Derek Phillips*, Stanford University
&lt;br&gt;&lt;br&gt;
53.   Application for Travel Grant, Samuel Fufa*, NA
&lt;br&gt;&lt;br&gt;
54.   Gender classification using facial components, Mayibongwe Bayana*, University of Kwazulu Natal
&lt;br&gt;&lt;br&gt;
55.   Noisy Expectation-Maximization: Applications and Generalizations, Osonde Osoba*, RAND Corporation
&lt;br&gt;&lt;br&gt;
56.   SEGCloud: Semantic Segmentation of 3D Point Clouds, Lyne Tchapmi*, Stanford University; Christopher Choy, Stanford University; Iro Armeni, Stanford University; JunYoung Gwak, Stanford University; Silvio Savarese, Stanford University
&lt;br&gt;&lt;br&gt;
57.   The Promise and Peril of Human Evaluation for Model Interpretability, Bernease Herman*, University of Washington
&lt;br&gt;&lt;br&gt;
58.   Adversarial Functionality-Preserving Training in the Malware Domain, Ousmane Dia*, ElementAI
&lt;br&gt;&lt;br&gt;
59.   Synchronized Video and Motion Capture Dataset and Quantitative Evaluation of Vision Based Skeleton Tracking Methods for Robotic Action Imitation, selamawet atnafu*, Bahirdar University
&lt;br&gt;&lt;br&gt;
60.   Constrained Dominant Sets with Applications in Computer Vision, Alemu Leulseged*, Ca’ Foscari University of Venice
&lt;br&gt;&lt;br&gt;
61.   Generalization Properties of Adaptive Gradient Methods in Machine Learning, Ashia Wilson*, UC Berkeley
&lt;br&gt;&lt;br&gt;
62.   Nods and Daps: Encouraging Gesture, Movement Rhythm &amp; Motion that honors the black experience and in the creation of Data Sets that drive AI, Micah Morgan*, African American Art and Culture Complex
&lt;br&gt;&lt;br&gt;
63.   Collecting Data in VR For Generating Natural Language Descriptions of 3D Space, Danielle Olson*, MIT
&lt;br&gt;&lt;br&gt;
64.   AWE-CM Vectors: Augmenting Word Embeddings with a Clinical Metathesaurus, Mohamed Kane-Hassan
&lt;br&gt;&lt;br&gt;
65.   Convolutional Neural Networks for Breast Cancer Screening: Transfer Learning with Exponential Decay, Hiba CHOUGRAD
&lt;br&gt;&lt;br&gt;
66.   A comparison of the conditional inference survival forest model to random survival forests based on a simulation study as well on two applications with time-to-event data, Justine Nasajje
&lt;br&gt;&lt;br&gt;
67.   Automated detection of Malaria Parasites using CNN via Smartphones, Sanni Oluwatoyin Yetunde
&lt;br&gt;&lt;br&gt;
68.   Using Machine Learning to Detect Potential Child Suicide Bombers, Cisca Oladipo
&lt;br&gt;&lt;br&gt;
69.   Reducing Students Dropout Rate - A machine Learning Approach, Neema Mduma
&lt;br&gt;&lt;br&gt;
70.   Generating Natural Language Descriptions of Virtual Reality (VR) Spaces, Danielle Olson
&lt;br&gt;&lt;br&gt;
71.   Social Attention for Part-of-Speech Tagging, Taha Merghani
&lt;br&gt;&lt;br&gt;
72.   Automatic Radio Galaxy Classification using Deep Convolutional Neural Networks, Wathela Alhassan, University of Cape Town; R. Taylor, University of Cape Town, University of the Western Cape; Mattia Vaccari, University of the Western Cape
&lt;br&gt;&lt;br&gt;
73.   Dynamic Modelling of Cybercriminals Behaviour by Deep Neural Networks, Abiodun Modupe*
&lt;br&gt;&lt;br&gt;
74.   Big data clustering with the use of the random projection features reduction and collaborative Fuzzy C-Means, Dang Trong Hop, Hanoi University of Industry; Pham The Long, Le Quy Don Technical University; Ngo Thanh Long, Le Quy Don Technical University; Fadugba Jeremiah, FPT University
&lt;br&gt;&lt;br&gt;
75.   Orchestra Mobile Crowdsensing and Computing Platform: A Roadmap for Further Development, Sando George, Warsaw University of Technology; Maria Ganzha, Warsaw University of Technology; Marcin Paprzycki, Polish Academy of Sciences
&lt;br&gt;&lt;br&gt;
76.   An empirical experimental survey of application of Wilson’s edited Nearest Neighbour as a sampling and data reduction scheme to alleviate class imbalance problem,  S. O. Folorunso, Olabisi Onabanjo University; A. B. Adeyemo, University of Ibadan
&lt;br&gt;&lt;br&gt;
77.   Luganda Text-to-Speech Machine, Irene Nandutu, Uganda Technology and Management University; Ernest Mwebaze, Makerere University
&lt;br&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programs 2018</title>
      <link>https://esube.github.io/workshop/2018/programs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/programs/</guid>
      <description>

&lt;h1 id=&#34;schedule-2018&#34;&gt;Schedule 2018&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Event&lt;/th&gt;
&lt;th&gt;Speaker&lt;/th&gt;
&lt;th&gt;Institution&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;09:00-09:10&lt;/td&gt;
&lt;td&gt;Opening Remarks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;BAI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;09:10-09:45&lt;/td&gt;
&lt;td&gt;Keynote 1&lt;/td&gt;
&lt;td&gt;Yann Dauphin&lt;/td&gt;
&lt;td&gt;Facebook&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;09:45-10:00&lt;/td&gt;
&lt;td&gt;Oral 1&lt;/td&gt;
&lt;td&gt;Sicelukwanda Zwane&lt;/td&gt;
&lt;td&gt;University of the Witwatersrand&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10:00-10:15&lt;/td&gt;
&lt;td&gt;Oral 2&lt;/td&gt;
&lt;td&gt;Alvin Grissom II&lt;/td&gt;
&lt;td&gt;Ursinus College&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10:15-10:30&lt;/td&gt;
&lt;td&gt;Oral 3&lt;/td&gt;
&lt;td&gt;Obioma Pelka&lt;/td&gt;
&lt;td&gt;University of Duisburg-Essen Germany&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10:30-11:00&lt;/td&gt;
&lt;td&gt;Coffee Break + poster&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11:00-11:35&lt;/td&gt;
&lt;td&gt;Keynote 2&lt;/td&gt;
&lt;td&gt;Ayanna Howard&lt;/td&gt;
&lt;td&gt;Georgia Institute of Technology&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11:35-11:50&lt;/td&gt;
&lt;td&gt;Oral 4&lt;/td&gt;
&lt;td&gt;Randi Williams&lt;/td&gt;
&lt;td&gt;MIT Media Lab&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11:50-12:05&lt;/td&gt;
&lt;td&gt;Oral 5&lt;/td&gt;
&lt;td&gt;Justice Amoh&lt;/td&gt;
&lt;td&gt;Dartmouth College&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12:05-12:20&lt;/td&gt;
&lt;td&gt;Oral 6&lt;/td&gt;
&lt;td&gt;Kehinde Owoeye&lt;/td&gt;
&lt;td&gt;University College London&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12:20-02:15&lt;/td&gt;
&lt;td&gt;Lunch + Poster&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;02:15-02:30&lt;/td&gt;
&lt;td&gt;Oral 7&lt;/td&gt;
&lt;td&gt;Lucio Dery&lt;/td&gt;
&lt;td&gt;Facebook Inc. / Stanford University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;02:30-02:45&lt;/td&gt;
&lt;td&gt;Oral 8&lt;/td&gt;
&lt;td&gt;Inioluwa Deborah Raji&lt;/td&gt;
&lt;td&gt;University of Toronto&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;02:45-03:00&lt;/td&gt;
&lt;td&gt;Oral 9&lt;/td&gt;
&lt;td&gt;Tewodros Gebreselassie&lt;/td&gt;
&lt;td&gt;Addis Ababa University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;03:00-03:15&lt;/td&gt;
&lt;td&gt;Oral 10&lt;/td&gt;
&lt;td&gt;Raesetje Sefala&lt;/td&gt;
&lt;td&gt;Wits University&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;03:15-03:45&lt;/td&gt;
&lt;td&gt;Coffee Break + poster&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;03:45-04:20&lt;/td&gt;
&lt;td&gt;Keynote 4&lt;/td&gt;
&lt;td&gt;Brittny-Jade Saunders&lt;/td&gt;
&lt;td&gt;NYC Commission on Human Rights&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;04:20-04:55&lt;/td&gt;
&lt;td&gt;Keynote 5&lt;/td&gt;
&lt;td&gt;Terrence Wilkerson&lt;/td&gt;
&lt;td&gt;Entrepreneur&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;04:55-05:25&lt;/td&gt;
&lt;td&gt;Panel Discussion on AI Ethics&lt;/td&gt;
&lt;td&gt;Ezinne Nwankwo (moderator), Stephanie Dinkins, Ayanna Howard, Brittny Saunders, and Terrance Wilkerson&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;05:25-05:30&lt;/td&gt;
&lt;td&gt;Closing Remarks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;td colspan=3&gt; &lt;b&gt; Dinner Schedule &lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;07:00-07:30&lt;/td&gt;
&lt;td&gt;Reception&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;07:30-08:00&lt;/td&gt;
&lt;td&gt;Welcome to Dinner&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;08:00-10:00&lt;/td&gt;
&lt;td&gt;Dinner&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;08:30-08:45&lt;/td&gt;
&lt;td&gt;BAI presentations&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;08:45-09:00&lt;/td&gt;
&lt;td&gt;Keynote 1&lt;/td&gt;
&lt;td&gt;Stephanie Dinkins&lt;/td&gt;
&lt;td&gt;Stony Brook University, Data &amp;amp; Society&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;09:00-09:15&lt;/td&gt;
&lt;td&gt;Keynote 2&lt;/td&gt;
&lt;td&gt;Karim Beguir&lt;/td&gt;
&lt;td&gt;InstaDeep&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;09:15-09:30&lt;/td&gt;
&lt;td&gt;Keynote 3&lt;/td&gt;
&lt;td&gt;Vukosi Marivate&lt;/td&gt;
&lt;td&gt;University of Pretoria / CSIR&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10:00-02:00 am&lt;/td&gt;
&lt;td&gt;Dancing/Music&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;keynote-speakers&#34;&gt;Keynote Speakers&lt;/h1&gt;

&lt;hr&gt;

&lt;h2 id=&#34;ayanna-howard-http-howard-ece-gatech-edu&#34;&gt;&lt;a href=&#34;http://howard.ece.gatech.edu/&#34; target=&#34;_blank&#34;&gt;Ayanna Howard&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/ayana.png&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Ensuring a Better World through Engineering, AI and Yes - ROBOTS&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: Ayanna Howard, Ph.D. is the Linda J. and Mark C. Smith Professor and Chair of the School of Interactive Computing at the Georgia Institute of Technology. Dr. Howard’s career focus is on intelligent technologies that must adapt to and function within a human-centered world. Her work, which encompasses advancements in artificial intelligence (AI), assistive technologies, and robotics, has resulted in over 250 peer-reviewed publications in a number of projects - from healthcare robots in the home to AI-powered STEM apps.  To date, her unique accomplishments have been highlighted through a number of awards and articles, including highlights in USA Today, Upscale, and TIME Magazine, as well as being recognized as one of the 23 most powerful women engineers in the world by Business Insider. In 2013, she also founded Zyrobotics, which develops STEM educational products to engage children of all abilities.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;brittny-jade-saunders-https-www1-nyc-gov-site-cchr-about-our-team-page&#34;&gt;&lt;a href=&#34;https://www1.nyc.gov/site/cchr/about/our-team.page&#34; target=&#34;_blank&#34;&gt;Brittny-Jade Saunders&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/brittny.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Local Government &amp;amp; the Challenge of Algorithmic Accountability &amp;ldquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: Brittny Saunders is Deputy Commissioner for Strategic Initiatives at the NYC Commission on Human Rights (“Commission”). At the Commission, Brittny manages key inter-agency partnerships and special projects related to data-driven discrimination and racial justice among others. Before joining the Commission, Brittny worked for the Office of the Mayor, most recently as Acting Counsel to the Mayor. Prior to that, as Deputy Counsel, Brittny played a central role in the Office’s broadband equity efforts, working to ensure affordable access to high-speed internet for residents of the five boroughs. Before joining local government, Brittny worked for the Center for Popular Democracy (“CPD”), where she was Supervising Attorney for Immigrant Rights and Racial Justice, and as Senior Advocate at the Center for Social Inclusion (“CSI” now “RaceForward”). Brittny was a 2010 Fellow in Media, Information &amp;amp; Communications Policy with the Rockwood Leadership Institute and a 2018 Wasserstein Fellow at Harvard Law School. She has an A.B. from Harvard College and a J.D. from Harvard Law School.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: Brittny will provide an overview of the work of the New York City Commission on Rights, the New York City Human Rights Law and the agency&amp;rsquo;s developing work on data-driven discrimination.  In addition, she will speak about her work on other local government efforts at the intersection of human rights and emerging technologies.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;karim-beguir-https-www-linkedin-com-in-karim-beguir-2350161&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/karim-beguir-2350161/&#34; target=&#34;_blank&#34;&gt;Karim Beguir&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/karim.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Life of an ML Startup&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: Karim helps companies get a grip on the latest AI breakthroughs and deploy them. A graduate of France’s Ecole Polytechnique and former Program Fellow at the Courant Institute in New York, Karim has a passion for teaching and using applied mathematics.&lt;/p&gt;

&lt;p&gt;This led him to launch InstaDeep, a fast-growing African AI startup that focuses on decision making for the Enterprise. Nominated at the MWC17 in the Top 20 most intriguing startups by PCMAG, InstaDeep now has offices in Tunis, London, Paris and Nairobi.&lt;/p&gt;

&lt;p&gt;Karim is also the founder of the TensorFlow Tunis Meetup and a Google Developer Expert in ML. He regularly organizes educational events and workshops to share his experience with the community, including mentoring in ML at Google Launchpad Accelerator Africa. Karim is on a mission to democratise AI and make it accessible to a wide audience.&lt;/p&gt;

&lt;h2 id=&#34;stephanie-dinkins-http-www-stephaniedinkins-com&#34;&gt;&lt;a href=&#34;http://www.stephaniedinkins.com/&#34; target=&#34;_blank&#34;&gt;Stephanie Dinkins&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/stephanie.png&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;:&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;terrence-wilkerson&#34;&gt;Terrence Wilkerson&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/terrence.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: Terrence is a family man and the proud father of four daughters. Born and raised in the Bronx, Terrence first encountered the criminal legal system at a young age. He was twice wrongfully accused of crimes he did not commit: once at 19 and again at 40. He is eager to share lessons from those experiences.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;vukosi-marivate-http-www-vima-co-za&#34;&gt;&lt;a href=&#34;http://www.vima.co.za/&#34; target=&#34;_blank&#34;&gt;Vukosi Marivate&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/vukosi-marivate.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Building the bridge&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: Vukosi holds a PhD in Computer Science (Rutgers University) and MSc &amp;amp; BSc in Electrical Engineering (Wits University). He has recently started at the University of Pretoria as the ABSA Chair of Data Science. Vukosi works on developing Machine Learning/Artificial Intelligence methods to extract insights from data. A large part of his work over the last few years has been in the intersection of  Machine Learning and Natural Language Processing(due to the abundance of text data and need to extract insights). As part of his vision for the ABSA Data Science chair, Vukosi is interested in Data Science for Social Impact, using local challenges as a springboard for research. In this area Vukosi has worked on projects in science, energy, public safety and utilities. Vukosi is an organizer of the Deep Learning Indaba, the largest Machine Learning/Artificial Intelligence workshop on the African continent, aiming to strengthen African Machine Learning. He is passionate about developing young talent, supervising MSc and PhD students and mentoring budding Data Scientists.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;yann-dauphin-http-www-dauphin-io&#34;&gt;&lt;a href=&#34;http://www.dauphin.io/&#34; target=&#34;_blank&#34;&gt;Yann Dauphin&lt;/a&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/yann.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Bio&lt;/b&gt;: His research focuses on understanding and developing deep learning algorithms. These algorithms are already helping make our lives better, but they could also help us understand ourselves. He is interested in creating deep learning algorithms that can learn with little supervision and to understand the principles of learning. He completed his Ph.D. at U. of Montreal with Yoshua Bengio on the subject of scaling deep learning algorithms. His collaborators and him have won two international AI competitions: the Unsupervised Transfer Learning Challenge in 2011, and the EmotiW challenge in 2014.&lt;/p&gt;

&lt;h1 id=&#34;oral-research-presenters&#34;&gt;Oral Research Presenters&lt;/h1&gt;

&lt;hr&gt;

&lt;h2 id=&#34;alvin-grissom-ii-https-www-ursinus-edu-live-profiles-3125-alvin-grissom-ii&#34;&gt;&lt;a href=&#34;https://www.ursinus.edu/live/profiles/3125-alvin-grissom-ii&#34; target=&#34;_blank&#34;&gt;Alvin Grissom II&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Pathologies of Neural Models Make Interpretations Difficult&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: One way to interpret neural model predictions
is to highlight the most important input
features—for example, a heatmap visualization
over the words in an input sentence.
In existing interpretation methods for
NLP, a word’s importance is determined by
either input perturbation—measuring the decrease
in model confidence when that word is
removed—or by the gradient with respect to
that word. To understand the limitations of
these methods, we use input reduction, which
iteratively removes the least important word
from the input. This exposes pathological behaviors
of neural models: the remaining words
appear nonsensical to humans and are not the
ones determined as important by interpretation
methods. As we confirm with human experiments,
the reduced examples lack information
to support the prediction of any label,
but models still make the same predictions
with high confidence. To explain these
counterintuitive results, we draw connections
to adversarial examples and confidence calibration:
pathological behaviors reveal difficulties
in interpreting neural models trained with
maximum likelihood. To mitigate their deficiencies,
we fine-tune the models by encouraging
high entropy outputs on reduced examples.
Fine-tuned models become more interpretable
under input reduction without accuracy loss on
regular examples.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;inioluwa-deborah-raji-https-www-linkedin-com-in-deborah-raji-065751b2&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/deborah-raji-065751b2/&#34; target=&#34;_blank&#34;&gt;Inioluwa Deborah Raji&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;In the Shadow of Gender Shades&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: While there have been mounting calls for algorithmic transparency as more artificial intelligence services become mainstream, and audit
approaches for online platforms have been proposed, audit
strategies for the effective design and disclosure of external evaluations
of commercial pretrained machine learning models distributed
as Application Program Interfaces (APIs) remains underdeveloped.
This paper thus extends scholarship on the development and impact
of black-box algorithmic auditing by exploring a real-world
commercial facial analysis intersectional audit and the corporate
reactions to the audit release. This paper 1) outlines the audit design and the public and private audit disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from originally targeted companies T-1, T-2, T-3 on the Pilot Parliaments Benchmark as of August
2018, 3) provides performance results on PPB by non-target companies
NT-A and NT-B and 4) Explores differences in company
responses as shared through corporate communication.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;justice-amoh-http-justiceamoh-github-io&#34;&gt;&lt;a href=&#34;http://justiceamoh.github.io/&#34; target=&#34;_blank&#34;&gt;Justice Amoh&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;An Optimized Recurrent Unit for Ultra-Low Power Acoustic Event Detection&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: Currently, there is a demand for neural network models that can run on sensors, wearables and IoT devices. However, resource constraints of such edge devices make it challenging to realize on-device neural network inferencing. For ultra-low power wearable applications, there are no practical solutions for deploying neural networks. To meet this need, our work introduces a new recurrent unit architecture that is specifically adapted for on-device low-power acoustic event detection (AED). The proposed embedded Gated Recurrent Unit (eGRU) is based on the GRU architecture but features optimizations that make it implementable on ultra-low power micro-controllers such as the ARM Cortex M0+. With our proposed modifications, eGRU is demonstrated to be effective, especially for short duration AED and keyword spotting tasks. A single eGRU cell is 60x faster and 12x smaller than a GRU cell. Despite its optimizations, eGRU compares well with conventional GRU across AED tasks of different complexities.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;kehinde-owoeye-http-www-cs-ucl-ac-uk-people-k-owoeye-html&#34;&gt;&lt;a href=&#34;http://www.cs.ucl.ac.uk/people/K.Owoeye.html/&#34; target=&#34;_blank&#34;&gt;Kehinde Owoeye&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Identifying sheep with abnormal movement trajectory in a flock&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: Learning to identify anomalies in the behavior of individuals is becoming increasingly important for a variety of reasons for example in studying the progression of several diseases. Due to the need to assess the efficacy of therapeutic interventions, animals with longer life span are becoming increasingly important for assessing the efficacy of therapeutic interventions. In this presentation, I will describe computational methods that allow for the automatic discrimination of sheep with a genetic mutation that causes Batten disease from an age-matched control group, using GPS movement traces as input. Batten disease is a neurodegenerative disease with symptoms that are likely to affect the way that those with it move and socialize, including loss of vision and dementia. The distance covered in each ten minute period and, more specifically, outliers in each period, are used as the basis for identification. Our results show that, despite the variability in the sample, the bulk of the outliers during the period of observation came from the sheep with Batten disease. Our results, though preliminary, point towards the potential of using relatively simple movement metrics in identifying the onset of a phenotype in symptomatically similar conditions.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;lucio-dery-https-www-linkedin-com-in-lucio-dery&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/lucio-dery/&#34; target=&#34;_blank&#34;&gt;Lucio Dery&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Audio to Body Dynamics&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: We present a method that gets as input an audio of violin
or piano playing, and outputs a video of skeleton predictions
which are further used to animate an avatar. The
key idea is to create an animation of an avatar that moves
their hands similarly to how a pianist or violinist would do,
just from audio. Aiming for a fully detailed correct arms
and fingers motion is a goal, however, it’s not clear if body
movement can be predicted from music at all. In this paper,
we present the first result that shows that natural body dynamics
can be predicted at all. We built an LSTM network
that is trained on violin and piano recital videos uploaded to
the Internet. The predicted points are applied onto a rigged
avatar to create the animation.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;obioma-pelka-https-www-fh-dortmund-de-pelka&#34;&gt;&lt;a href=&#34;https://www.fh-dortmund.de/pelka/&#34; target=&#34;_blank&#34;&gt;Obioma Pelka&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Radiology Objects in COntext (ROCO): A Multimodal Medical Image Dataset&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: This work introduces a new multimodal image dataset, with the aim of detecting the interplay between visual elements and semantic relations present in radiology images. The objective is accomplished by retrieving all image-caption pairs from the open-access biomedical literature database PubMedCentral, as these captions describe the visual content in their semantic context. The target domain being radiology, all compound, multi-pane, and non-radiology images were eliminated using an automatic binary classifier fine-tuned with a deep convolutional neural network system and trained with datasets distributed at the medical tasks of ImageCLEF 2013, 2015 and 2016. The Radiology Objects in COntext (ROCO) dataset contains over 81k radiology images with several medical imaging modalities including Computer Tomography (CT), Ultrasound, X-Ray, Fluoroscopy, Positron Emission Tomography (PET), Mammography, Magnetic Resonance Imaging (MRI), Angiography and PET-CT. For all images in ROCO, the corresponding caption, keywords, Unified Medical Language Systems (UMLS) Concept Unique Identifiers (CUIs) and Semantic Type will be distributed. An additional out-of-class set with 6k images ranging from synthetic radiology figures to digital arts is provided, to improve prediction and classification performance of out-of-class samples. Adopting ROCO, systems for caption and keywords generation can be modeled, which enables multimodal representation for image datasets lacking text representation. Furthermore, systems with the goal of image structuring and semantic information tagging can be created using ROCO, which is beneficial and of assistance for image and information retrieval purposes.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;raesetje-sefala-https-www-linkedin-com-in-raesetje-sefala-2b9393119&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/raesetje-sefala-2b9393119/&#34; target=&#34;_blank&#34;&gt;Raesetje Sefala&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Using satellite images and computer vision to study the evolution and effects of spatial apartheid in South Africa&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: One of the main problems in South Africa is removing many of the legacies of Apartheid - a former policy of economic and political discrimination, and segregation against non-European groups in South Africa. For example, moving around residential areas shows the legacy of spatial apartheid on a smaller scale- completely segregated neighborhoods of townships next to gated wealthy neighborhoods that have largely remained unaffected by the ending of apartheid. Our research proposes to use computer vision to analyze millions of such satellite images of South Africa from 2006 to 2016. This work aims to use satellite images, geographically labelled coordinates of South Africa&amp;rsquo;s built environment and socioeconomic data to understand the relationship between the spatial and socioeconomic makeup of neighborhoods in South Africa, and study how they have evolved over time post Apartheid. We propose a semantic segmentation model to detect and classify clusters of townships and wealthy areas from these high resolution satellite images. In addition to automatically detecting and classifying neighborhoods, we plan to use demographic and socioeconomic data to then analyze the change over time in the demographic makeup, economic status and access to basic resources such as the number of clinics and schools of these detected neighborhoods.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;randi-williams-https-www-media-mit-edu-people-randiw12-overview&#34;&gt;&lt;a href=&#34;https://www.media.mit.edu/people/randiw12/overview/&#34; target=&#34;_blank&#34;&gt;Randi Williams&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;PopBots: Leveraging Social Robots to Aid Early Childhood Artificial Intelligence Education&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: Artificial intelligence (AI) is revolutionizing our lives, impacting the way that even the youngest members of society live, learn, and play. Previous work examining children’s relationships with AI has shown that this population lacks an understanding of how AI devices work. This lack of understanding makes it difficult for children to engage in safe and constructive interactions with their smart playthings. Furthermore, as this technology becomes more pervasive, we must think about how to build a diverse workforce that creates technology to equitably address the needs of many. Given these motivations, we designed an early childhood AI curriculum, PopBots. PopBots is a hands-on toolkit that enables young children to learn about AI by programming and training a social robot. We evaluated the toolkit with 80 preschool children (ages 4-6) and found that the use of a social robot as a learning companion and programmable artifact was effective. Children could correctly answer questions about knowledge-based systems, supervised machine learning, and generative music algorithms. Additionally, we found that using the toolkit helped children better appreciate the cognitive capabilities of robots. We will discuss the toolkit and teaching methods used in hope that this first exploration into early AI education will inspire other educators and researchers.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;sicelukwanda-zwane-https-www-linkedin-com-in-sicelukwanda-zwane-54873398&#34;&gt;&lt;a href=&#34;https://www.linkedin.com/in/sicelukwanda-zwane-54873398/&#34; target=&#34;_blank&#34;&gt;Sicelukwanda Zwane&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Safer Exploration in Deep Reinforcement Learning using Action Priors&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: Behavior learning in deep reinforcement learning is inherently unsafe because untrained agents typically have to sample actions from randomly initialized task policies and from random exploration policies. Executing these actions in physical environments can lead agents to harmful states, possibly causing damage and poor initial performance. In this work, we address this problem by using transfer learning to develop a framework for safer reinforcement learning in continuous environments. We show that our exploration policy results in fewer collisions with the environment, better initial performance, and earlier convergence compared to the vanilla epsilon-greedy random exploration policy.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;tewodros-abebe-gebreselassie-https-github-com-tewodrosabebe&#34;&gt;&lt;a href=&#34;https://github.com/TewodrosAbebe&#34; target=&#34;_blank&#34;&gt;Tewodros Abebe Gebreselassie&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Title&lt;/b&gt;: &amp;ldquo;Parallel Corpora for bi-Directional Statistical Machine Translation for Seven Ethiopian Language Pairs&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Abstract&lt;/b&gt;: In this paper, we describe the development of parallel corpora for Ethiopian Languages: Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. To check the usability of all the corpora we used them to conduct a baseline bi-directional statistical machine translation experiment for seven language pairs. The bi-directional SMT BLEU score shows that all the corpora can be used for further SMT investigations. We have also learnt that the morphological richness of the Ethio-Semitic languages has a negative impact on the performance of the SMT especially when they are target languages. Now we are working towards selecting an optimal alignment for bi-directional statistical machine translation among the Ethiopian languages.&lt;/p&gt;

&lt;h1 id=&#34;accepted-posters&#34;&gt;Accepted Posters&lt;/h1&gt;

&lt;p&gt;&lt;p style=&#34;color: black&#34;&gt;
(Accepted posters will be added soon)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sponsors 2017</title>
      <link>https://esube.github.io/workshop/2017/sponsors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/sponsors/</guid>
      <description>

&lt;p&gt;Thanks to our corporate sponsors, the workshop is free to attendees and we are able to provide inclusive travel funding to select participants.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;platinum-sponsors-black-power-in-ai&#34;&gt;Platinum Sponsors (Black Power in AI)&lt;/h2&gt;

&lt;!-- &lt;figure class=&#34;img-sponsor-icon&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/blackinai.png&#34; /&gt;


&lt;/figure&gt;
 --&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/facebook.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;!-- &lt;hr&gt; ## Gold Sponsors --&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/microsoft.png&#34; /&gt;


&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/google.png&#34; /&gt;


&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;gold-sponsors-system&#34;&gt;Gold Sponsors (System)&lt;/h2&gt;

&lt;!-- &lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/fa-code.png&#34; /&gt;


&lt;/figure&gt;
 --&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/deepmind.png&#34; /&gt;


&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;silver-sponsors-component&#34;&gt;Silver Sponsors (Component)&lt;/h2&gt;

&lt;!-- &lt;figure class=&#34;img-sponsor-third-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/fa-chip.png&#34; /&gt;


&lt;/figure&gt;
 --&gt;

&lt;figure class=&#34;img-sponsor-third-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/elementai.png&#34; /&gt;


&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-third-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/airbnb.png&#34; /&gt;


&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-third-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/savoy.jpg&#34; /&gt;


&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-third-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/uber.png&#34; /&gt;


&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;supporters&#34;&gt;Supporters&lt;/h2&gt;

&lt;p&gt;We thank &lt;a href=&#34;https://b4capitalgroup.com/&#34; target=&#34;_blank&#34;&gt;B4 Capital Group&lt;/a&gt; for their support
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;We also thank the following institutions  for sponsoring their students to attend the  workshop&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cornell University&lt;/li&gt;
&lt;li&gt;Duke University&lt;/li&gt;
&lt;li&gt;Harvard University&lt;/li&gt;
&lt;li&gt;Stanford University&lt;/li&gt;
&lt;li&gt;University of California, Berkeley&lt;/li&gt;
&lt;li&gt;University of Illinois at Urbana-Champaign&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sponsors 2018</title>
      <link>https://esube.github.io/workshop/2018/sponsors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/sponsors/</guid>
      <description>

&lt;p&gt;Thanks to our corporate sponsors, the workshop is free to attendees and we are able to provide inclusive travel funding to select participants.&lt;/p&gt;

&lt;hr&gt;

&lt;h2 id=&#34;span-style-color-lightblue-diamond-sponsors-span&#34;&gt;&lt;span style=&#34;color:lightblue&#34;&gt;Diamond Sponsors&lt;/span&gt;&lt;/h2&gt;

&lt;!--&lt;figure class=&#34;img-sponsor-icon&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/blackinai.png&#34; /&gt;


&lt;/figure&gt;
 --&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://research.fb.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/facebook.jpg&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://ai.google/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/google.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://ai.intel.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/intel.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://www.waltonfamilyfoundation.org/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/walton-family.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;span-style-color-silver-platinum-sponsors-span&#34;&gt;&lt;span style=&#34;color:Silver&#34;&gt;Platinum Sponsors&lt;/span&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://www.research.ibm.com/ai/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/ibm.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://amazon.jobs/NIPS&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/amazon.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://www.salesforce.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/salesforce.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://eng.uber.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/uber.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://www.apple.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/apple.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;span-style-color-gold-gold-sponsors-span&#34;&gt;&lt;span style=&#34;color:Gold&#34;&gt; Gold Sponsors &lt;/span&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://www.nvidia.com/page/home.html&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/nvidia.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://www.deepmind.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/deepmind.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;http://www.microsoft.com/research&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/microsoft.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;hr&gt;

&lt;h2 id=&#34;span-style-color-silver-silver-sponsors-span&#34;&gt;&lt;span style=&#34;color:Silver&#34;&gt;Silver Sponsors&lt;/span&gt;&lt;/h2&gt;

&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://www.etsy.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/etsy.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://www.elementai.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/elementai.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;
&lt;a href=&#34;https://www.pinterest.com/&#34; target=&#34;_blank&#34;&gt;
&lt;img src=&#34;https://esube.github.io/img/pinterest.png&#34; /&gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;hr&gt;

&lt;!--&lt;hr&gt;

&lt;!-- We thank [B4 Capital Group](https://b4capitalgroup.com/) for their support
&lt;br&gt;&lt;br&gt; --&gt;

&lt;!-- &lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/airbnb.png&#34; /&gt;


&lt;/figure&gt;


&lt;figure class=&#34;img-sponsor-c&#34;&gt;

&lt;img src=&#34;https://esube.github.io/img/kaggle.png&#34; /&gt;


&lt;/figure&gt;
 --&gt;

&lt;p&gt;We thank the following companies and institutions for their support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.airbnb.com/&#34; target=&#34;_blank&#34;&gt;Airbnb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mila.quebec/en/&#34; target=&#34;_blank&#34;&gt;MILA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/instadeep/&#34; target=&#34;_blank&#34;&gt;InstaDeep&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vectorinstitute.ai/&#34; target=&#34;_blank&#34;&gt;Vector Institute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;p&gt;We also thank the following institutions for sponsoring their students to attend the  workshop&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Berkley University&lt;/li&gt;
&lt;li&gt;Cornell University&lt;/li&gt;
&lt;li&gt;Duke University&lt;/li&gt;
&lt;li&gt;Georgia Institute of Technology (Georgia Tech)&lt;/li&gt;
&lt;li&gt;Harvard University&lt;/li&gt;
&lt;li&gt;Massachusetts Institute of Technology (MIT)&lt;/li&gt;
&lt;li&gt;McGill University&lt;/li&gt;
&lt;li&gt;Northwestern University&lt;/li&gt;
&lt;li&gt;Rutgers University&lt;/li&gt;
&lt;li&gt;Stanford University, Computer Science&lt;/li&gt;
&lt;li&gt;Stanford University, Stats&lt;/li&gt;
&lt;li&gt;University of California, Berkeley&lt;/li&gt;
&lt;li&gt;University of Illinois at Urbana-Champaign&lt;/li&gt;
&lt;li&gt;University of Montreal, Montreal Institute for Learning Algorithms (MILA)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Submission Instructions 2017</title>
      <link>https://esube.github.io/workshop/2017/submissions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/submissions/</guid>
      <description>&lt;p&gt;We welcome theoretical, empirical, and applied work in machine learning and artificial intelligence, including, but not limited to, search, planning, knowledge representation, reasoning, natural language processing, computer vision, robotics, multiagent systems, statistical reasoning, and deep learning. Work may be previously published, completed, or ongoing. Submissions will be peer-reviewed by at least 2 reviewers. The workshop will not publish proceedings. The presenter must be a Black researcher in AI, and does not need to be first author.&lt;/p&gt;

&lt;p&gt;Submissions can be up to two page abstracts and must state the research problem, motivation, and technical contribution. Submissions must be self-contained and include all figures, tables, and references.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Submission page: &lt;a href=&#34;https://cmt3.research.microsoft.com/BLACKINAI2017&#34; target=&#34;_blank&#34;&gt;Black in AI CMT Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Travel Grants: In order to be considered for a travel grant, please select yes to the question Do you need a travel grant?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Submission Instructions 2018</title>
      <link>https://esube.github.io/workshop/2018/submissions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/submissions/</guid>
      <description>&lt;p&gt;We welcome work in artificial intelligence, including, but not limited to, computer vision, deep learning, knowledge reasoning, machine learning, multi-agent systems, natural language processing, statistical reasoning, theory, robotics, as well as applications of AI to other domains such as health and education, and submissions concerning fairness, ethics, and transparency in AI. Papers may introduce new theory, methodology, or applications. We also welcome position papers and demos related to these areas. Work may be previously published, completed, or ongoing. Submissions will be peer-reviewed by at least 2 reviewers in the area. The workshop will not publish proceedings. We encourage all Black researchers in the field to submit their work, and do not need to be first author of the work.&lt;br /&gt;
&lt;br&gt;
Submissions may be up to two pages including all figures and tables, with an additional page for references. The submissions should be in a single column. They should be typeset using 11-point or larger fonts and should have at least 1-inch margin all around. Submissions that do not follow these guidelines risk being rejected without consideration of their merits.&lt;br /&gt;
&lt;br&gt;
Submissions must state the research problem, motivation, and technical contribution. Submissions must be self-contained and include all figures, tables, and references. The submission page can be found &lt;a href=&#34;https://cmt3.research.microsoft.com/BLACKINAI2018&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and the submission deadline is &lt;strong&gt;5:00 PM EST, August 30, 2018&lt;/strong&gt;. Please note that no extensions will be offered for submissions.&lt;/p&gt;

&lt;p&gt;Here are a set of good sample papers from 2017: &lt;a href=&#34;https://github.com/blackinai/blackinai.github.io/tree/master/papers&#34; target=&#34;_blank&#34;&gt;sample papers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Travel Grants 2017</title>
      <link>https://esube.github.io/workshop/2017/grants/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2017/grants/</guid>
      <description>

&lt;h2 id=&#34;travel-awards&#34;&gt;Travel Awards&lt;/h2&gt;

&lt;p&gt;Need-based travel grants will be awarded to workshop participants. The travel grant can be used for covering costs associated with the workshop such as NIPS registration, accommodation and travel. Please note that the travel grants may not cover all of your costs and we may not be able to award them to all applicants. The amount of money we award to each person will depend on the number of applicants and the location each applicant will be traveling from.&lt;/p&gt;

&lt;p&gt;If you are a student who has not conducted research and would like a travel grant to attend our workshop, you may do so by either:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Submitting an abstract, and a paragraph describing financial need on the conference submission page OR&lt;/li&gt;
&lt;li&gt;Submitting a one page statement describing your research interests in AI and reasons for participating in the workshop, and a paragraph describing financial need on the conference submission page&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These must be done by the deadline, &lt;strong&gt;October 13, 2017&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Travel Grants 2018</title>
      <link>https://esube.github.io/workshop/2018/grants/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esube.github.io/workshop/2018/grants/</guid>
      <description>&lt;p&gt;Need-based travel grants will be awarded to workshop participants. Participants do not need to submit a paper in order to be considered for a travel grant. The travel grant can be used for covering costs associated with the workshop such as NIPS registration, accommodation and travel. Please note that the travel grants may not cover all of your costs and we may not be able to award them to all applicants. The amount of money we award to each person will depend on the number of applicants and the location each applicant will be traveling from. Please submit your travel grant application by &lt;strong&gt;5:00 PM EST, August 24st&lt;/strong&gt; by filling out &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSfhjdWiCTPDneG-u226iXCxcXLy9sqDyEHjdeXO9X6vTDDAQw/viewform&#34; target=&#34;_blank&#34;&gt;the travel grant application form&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
